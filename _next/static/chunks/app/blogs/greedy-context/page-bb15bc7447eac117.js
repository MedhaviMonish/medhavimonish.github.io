(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[551],{61386:(e,s,t)=>{Promise.resolve().then(t.bind(t,53134))},53134:(e,s,t)=>{"use strict";t.r(s),t.d(s,{default:()=>l});var n=t(95155),a=t(41411),i=t(5565);function l(){return(0,n.jsxs)("main",{className:"relative min-h-screen bg-cover bg-center bg-no-repeat bg-fixed flex justify-center items-start py-20 px-4",style:{backgroundImage:"url('/images/bg-home.png')"},children:[(0,n.jsxs)("div",{className:"w-full max-w-4xl bg-black/80 text-gray-200 p-10 rounded-xl shadow-md",children:[(0,n.jsxs)("h1",{className:"text-4xl font-bold text-ember mb-2",children:[(0,n.jsx)("a",{href:"https://github.com/MedhaviMonish/GreedyContext",target:"_blank",rel:"noopener noreferrer",className:"text-blue-400 hover:underline text-base block mb-2",children:"\uD83D\uDD17 View on GitHub"}),"GreedyContext: Shrinking LLM Memory with Semantic Graphs"]}),(0,n.jsx)("p",{className:"mb-6 leading-relaxed",children:"Traditional LLM-based chat systems send the entire conversation history to the model, even if most of it is no longer relevant. This not only increases token usage and latency but can also confuse the model during topic switches. GreedyContext solves this by constructing a semantic graph of message relationships and then extracting a minimal, meaningful subset of messages that are directly relevant to the user's latest query. So instead of blindly sending 100+ previous messages, you send only the 5–10 that matter."}),(0,n.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"❌ Problems with Full-History Approach"}),(0,n.jsxs)("ul",{className:"list-disc list-inside space-y-2 text-gray-400 leading-relaxed mb-6",children:[(0,n.jsx)("li",{children:"Includes outdated or unrelated messages just because they're part of the session"}),(0,n.jsx)("li",{children:"Treats topic switches as continuation — when they should be fresh starts"}),(0,n.jsx)("li",{children:"Token usage grows linearly with conversation length"}),(0,n.jsx)("li",{children:"Drives up cost, latency, and risk of hallucination"}),(0,n.jsx)("li",{children:"Requires summarization layers or external memory management or custom memory modules"}),(0,n.jsx)("li",{children:"Makes it unclear which messages actually shaped the response"})]}),(0,n.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"✅ GreedyContext Advantages"}),(0,n.jsxs)("ul",{className:"list-disc list-inside space-y-2 text-gray-400 leading-relaxed mb-6",children:[(0,n.jsx)("li",{children:"Strips out irrelevant branches and topic switches — treating each as a fresh intent"}),(0,n.jsx)("li",{children:"Keeps only semantically linked messages, not just recent ones"}),(0,n.jsx)("li",{children:"Reduces tokens by 50–90%, slashing cost and improving focus"}),(0,n.jsx)("li",{children:"Avoids summarization yet keeps outputs coherent"}),(0,n.jsx)("li",{children:"Less LLM confusion = fewer hallucinations and context drift"}),(0,n.jsx)("li",{children:"Plug-and-play with any LLM — no infra or architecture change needed"})]}),(0,n.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"\uD83D\uDCCC A Simple Example"}),(0,n.jsx)("p",{className:"mb-6 leading-relaxed",children:"Imagine this is a real user conversation:"}),(0,n.jsxs)("ul",{className:"list-disc list-inside space-y-2 text-gray-400 leading-relaxed mb-6",children:[(0,n.jsx)("li",{children:"1️⃣ “I love taking long walks on the beach during sunset.”"}),(0,n.jsx)("li",{children:"2️⃣ “Artificial Intelligence is transforming the world rapidly.”"}),(0,n.jsx)("li",{children:"3️⃣ “Is there any financial aid or scholarship info I can look into?”"}),(0,n.jsx)("li",{children:"4️⃣ “I'm also curious about campus housing options—are freshmen required to live on campus?”"}),(0,n.jsx)("li",{children:"5️⃣ “By the way, do you know any good recommendations for online courses in data science?”"}),(0,n.jsx)("li",{children:"6️⃣ “How long does it typically take to complete one of those courses?”"}),(0,n.jsx)("li",{children:"7️⃣ “And for the application process, should I contact professors directly?”"}),(0,n.jsx)("li",{children:"8️⃣ “Are there any specific deadlines I should be aware of?”"}),(0,n.jsx)("li",{children:"9️⃣ “So, should I submit my test scores along with my application?”"}),(0,n.jsx)("li",{children:"\uD83D\uDD1F “What about letters of recommendation—how many do I need?”"})]}),(0,n.jsx)("p",{className:"mb-6 leading-relaxed",children:"In a traditional method, all 10 messages would be sent to the model — including unrelated ones about beaches, data science, and student life."}),(0,n.jsx)("p",{className:"mb-6 leading-relaxed",children:"With GreedyContext, only messages \uD83D\uDD1F, 9️⃣, 8️⃣, 7️⃣, and 3️⃣ are selected — because they are semantically related to the application and admissions topic. This drastically reduces token usage and focuses model attention on what actually matters."}),(0,n.jsxs)("p",{className:"mb-6 leading-relaxed",children:["If the user sends an 11",(0,n.jsx)("sup",{children:"th"})," message like ",(0,n.jsx)("b",{children:(0,n.jsx)("i",{children:"“What is LLM?”"})}),", then all previous 10 messages become irrelevant. This method ensures those irrelevant messages are skipped — greatly benefiting LLMs during long sessions."]}),(0,n.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"\uD83D\uDD0D What GreedyContext Does"}),(0,n.jsxs)("ul",{className:"list-disc list-inside space-y-2 text-gray-400 leading-relaxed mb-6",children:[(0,n.jsx)("li",{children:"Embeds all conversation messages using SentenceTransformers"}),(0,n.jsx)("li",{children:"Builds a similarity graph using cosine scores"}),(0,n.jsx)("li",{children:"Restricts connections to past messages (upper triangular)"}),(0,n.jsx)("li",{children:"Uses greedy search to find the most relevant backward chain"}),(0,n.jsx)("li",{children:"Returns just the chain — no summarization needed"})]}),(0,n.jsx)("p",{className:"mb-6 leading-relaxed",children:"The full graph visualization takes time because it generates HTML, but the actual filtering is fast — in under 0.5 seconds, hundreds of messages can be reduced to a tenth or less. This helps the LLM preserve context and respond faster by processing fewer tokens."}),(0,n.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"⚙️ The Code"}),(0,n.jsx)("p",{className:"mb-6 leading-relaxed",children:"The full code fits in one Python file. You embed, graph, and greedily walk back:"}),(0,n.jsx)(a.A,{language:"python",code:'from sentence_transformers import SentenceTransformer\nimport networkx as nx\nimport numpy as np\n\ns = ["message 1", "message 2", ..., "latest message"]\nmodel = SentenceTransformer("all-MiniLM-L6-v2")\nembeddings = model.encode(s)\nsimilarities = np.inner(embeddings, embeddings)\n\n# Strict upper triangular\nfor i in range(len(s)):\n  similarities[i, :i+1] = 0\n\nG = nx.DiGraph()\nfor i in range(len(s)):\n  for j in range(i+1, len(s)):\n    if similarities[i][j] > 0.2:\n      G.add_edge(j+1, i+1, weight=similarities[i][j])\n\n# Greedy backward walk\npath = [len(s)]\nwhile path[-1] != 1:\n  preds = list(G[path[-1]].items())\n  if not preds: break\n  next_node = max(preds, key=lambda x: x[1][\'weight\'])[0]\n  path.append(next_node)\npath.reverse()'}),(0,n.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"\uD83D\uDDBC️ Visualizing the Greedy Path"}),(0,n.jsx)("p",{className:"mb-4 leading-relaxed",children:"The red path below shows the semantic trail GreedyContext selects. We are tracking back from the latest user message to the first one. The first visualization uses no threshold. The next one applies a small threshold to filter weaker links. Gray edges represent all possible semantic links:"}),(0,n.jsx)(i.default,{src:"https://raw.githubusercontent.com/MedhaviMonish/GreedyContext/main/images/without_threshold.png",alt:"GreedyContext path visualization",width:600,height:600,className:"rounded shadow mb-6"}),(0,n.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"\uD83D\uDCA1 Final Tip"}),(0,n.jsxs)("p",{className:"mb-6 leading-relaxed",children:["For optimal results, filter out weak similarities (e.g., below 0.2). This makes the graph cleaner and the context chain sharper — like this:",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(i.default,{src:"https://raw.githubusercontent.com/MedhaviMonish/GreedyContext/main/images/with_threshold.png",alt:"GreedyContext path visualization",width:600,height:600,className:"rounded shadow mb-6"}),(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"\uD83D\uDCAC Bonus: You don’t need any database or summarization logic. Just plug this in and call your LLM with the selected messages."]})]}),(0,n.jsx)("button",{onClick:()=>window.scrollTo({top:0,behavior:"smooth"}),className:"fixed bottom-6 right-6 z-50 bg-ember text-black px-4 py-2 rounded-full shadow-lg hover:bg-orange-400 transition",children:"↑ Top"})]})}},41411:(e,s,t)=>{"use strict";t.d(s,{A:()=>r});var n=t(95155),a=t(64566),i=t(51263),l=t(12115);function r(e){let{code:s,language:t="text"}=e,[r,o]=(0,l.useState)(!1);return(0,n.jsxs)("div",{className:"relative mb-6 rounded-lg overflow-hidden",children:[(0,n.jsx)("span",{className:"absolute top-2 left-2 text-xs text-gray-300 px-2 py-0.5 rounded",children:t}),(0,n.jsx)("button",{onClick:()=>{navigator.clipboard.writeText(s),o(!0),setTimeout(()=>o(!1),1500)},className:"absolute top-2 right-2 text-xs text-black bg-ember px-2 py-1 rounded hover:bg-orange-500 transition z-10",children:r?"Copied!":"Copy"}),(0,n.jsx)(a.A,{language:t,style:i.hc,showLineNumbers:!0,customStyle:{padding:"1.75rem 1.25rem 0.25rem 1.25rem",fontSize:"0.9rem",background:"transparent"},children:s})]})}}},e=>{var s=s=>e(e.s=s);e.O(0,[432,565,441,517,358],()=>s(61386)),_N_E=e.O()}]);