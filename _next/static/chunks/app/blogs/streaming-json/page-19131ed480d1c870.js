(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[904],{47483:(e,t,s)=>{Promise.resolve().then(s.bind(s,75929))},75929:(e,t,s)=>{"use strict";s.r(t),s.d(t,{default:()=>i});var r=s(95155),a=s(41411),n=s(48173),l=s.n(n);function i(){return(0,r.jsx)("main",{className:"relative min-h-screen bg-cover bg-center bg-no-repeat bg-fixed flex justify-center items-start py-20 px-4",style:{backgroundImage:"url('/images/bg-home.png')"},children:(0,r.jsxs)("div",{className:"w-full max-w-4xl bg-black/80 text-white p-10 rounded-xl shadow-md",children:[(0,r.jsx)(l(),{href:"/",className:"text-sm text-gray-400 hover:text-white transition block mb-4",children:"← Back to Home"}),(0,r.jsxs)("h1",{className:"text-4xl font-bold text-ember mb-2",children:[(0,r.jsx)(l(),{href:"https://github.com/MedhaviMonish/Stream-JSON",target:"_blank",rel:"noopener noreferrer",className:"text-blue-400 hover:underline text-base block mb-2",children:"\uD83D\uDD17 View on GitHub"}),"Streaming JSON While Streaming from LLM"]}),(0,r.jsx)("p",{className:"mb-6 leading-relaxed",children:"When generating structured responses from LLMs, streaming JSON is almost impossible. JSON parsing requires strict syntax — every quote, comma, and bracket must be perfectly placed. But when you stream output token-by-token, the structure is often incomplete mid-way. A missing brace or an unclosed string can break the whole parser."}),(0,r.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"\uD83D\uDCA5 The Problem"}),(0,r.jsx)("p",{className:"mb-6 leading-relaxed",children:"JSON is strict. If you're using OpenAI's streaming API and trying to render JSON in real-time, you can't do much until the full structure is delivered. That means the UI can't update until the model stops thinking — and that's frustrating. If your JSON affects how UI is rendered then you have to wait until the end, unlike text where you can stream freely without structural constraints."}),(0,r.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"\uD83D\uDD27 The Trick: Use YAML"}),(0,r.jsx)("p",{className:"mb-6 leading-relaxed",children:"YAML is more forgiving. It doesn’t require commas, brackets, or perfect nesting at every step. Here's what I did:"}),(0,r.jsxs)("ul",{className:"list-disc list-inside space-y-2 text-gray-400 leading-relaxed mb-6",children:[(0,r.jsx)("li",{children:"Let the model output YAML instead of JSON"}),(0,r.jsx)("li",{children:"Stream it token-by-token"}),(0,r.jsx)("li",{children:"Try parsing it incrementally using a custom parser"}),(0,r.jsx)("li",{children:"Merge whatever is valid into a full JSON template"})]}),(0,r.jsxs)("p",{className:"mb-6 leading-relaxed",children:["In this code I have used ",(0,r.jsx)("b",{children:"YamlTokenStreamer"})," to simulate token streaming. This class is purely for local testing — when using a real LLM with stream support (like OpenAI), you won’t need it. Just plug the rest of the logic into your streamed tokens, and you'll be able to produce fully structured JSON in real time — with `null` values where data is not yet available."]}),(0,r.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"\uD83D\uDCE6 The Template"}),(0,r.jsx)(a.A,{language:"json",code:'{\n  "user": {"id": null, "name": null, "roles": []},\n  "lastLogin": null\n}'}),(0,r.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"\uD83D\uDCE1 Real-Time Merge"}),(0,r.jsx)(a.A,{language:"python",code:"buffer = \"\"\nfor chunk in openai.ChatCompletion.create(..., stream=True):\n    token = chunk['choices'][0].delta.get('content')\n    if token:\n        buffer += token\n        parsed, _ = try_partial_yaml_parse(buffer)\n        merged = merge_yaml_into_template(template, parsed)\n        print(merged)  # Always structured, even when partial"}),(0,r.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"\uD83E\uDDE0 Bonus: Quote Heuristics"}),(0,r.jsx)("p",{className:"mb-6 leading-relaxed",children:'If parsing fails, the parser tries to append a missing quote `"` at the end of the last line — this fixes many broken outputs where the model stopped mid-string.'}),(0,r.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"\uD83D\uDCC8 Outcome"}),(0,r.jsxs)("p",{className:"mb-6 leading-relaxed",children:["Now ",(0,r.jsx)("b",{className:"text-ember",children:"YOU"})," can stream LLM output into your UI and render the structure in real-time — safely. Every key exists thanks to the template. Every value updates only when it's available. No more crashes due to malformed JSON."]}),(0,r.jsx)("h2",{className:"text-2xl font-bold text-ember mb-2",children:"\uD83C\uDFC1 Final Thought"}),(0,r.jsx)("p",{className:"mb-6 leading-relaxed",children:"YAML isn't just human-friendly — it's stream-friendly too. If you're building tool-based agents or structured UIs, this simple trick can make a big difference. Don't wait for perfect JSON. Start rendering now."})]})})}},41411:(e,t,s)=>{"use strict";s.d(t,{A:()=>i});var r=s(95155),a=s(64566),n=s(51263),l=s(12115);function i(e){let{code:t,language:s="text"}=e,[i,o]=(0,l.useState)(!1);return(0,r.jsxs)("div",{className:"relative mb-6 rounded-lg overflow-hidden",children:[(0,r.jsx)("span",{className:"absolute top-2 left-2 text-xs text-gray-300 px-2 py-0.5 rounded",children:s}),(0,r.jsx)("button",{onClick:()=>{navigator.clipboard.writeText(t),o(!0),setTimeout(()=>o(!1),1500)},className:"absolute top-2 right-2 text-xs text-black bg-ember px-2 py-1 rounded hover:bg-orange-500 transition z-10",children:i?"Copied!":"Copy"}),(0,r.jsx)(a.A,{language:s,style:n.hc,showLineNumbers:!0,customStyle:{padding:"1.75rem 1.25rem 0.25rem 1.25rem",fontSize:"0.9rem",background:"transparent"},children:t})]})}}},e=>{var t=t=>e(e.s=t);e.O(0,[932,441,517,358],()=>t(47483)),_N_E=e.O()}]);